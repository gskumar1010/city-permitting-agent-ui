export INFERENCE_MODEL="llama32"
export VLLM_URL="https://URL/v1"
export VLLM_TLS_VERIFY="false"
export VLLM_API_TOKEN="TOKEN"

oc create secret generic llama-stack-inference-model-secret \
  --from-literal INFERENCE_MODEL="$INFERENCE_MODEL" \
  --from-literal VLLM_URL="$VLLM_URL" \
  --from-literal VLLM_TLS_VERIFY="$VLLM_TLS_VERIFY" \
  --from-literal VLLM_API_TOKEN="$VLLM_API_TOKEN"
